{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70b39b1-0e80-474b-8358-ca5f6ab5fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Using cached gradio-6.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (4.10.0)\n",
      "Collecting audioop-lts<1.0 (from gradio)\n",
      "  Using cached audioop_lts-0.2.2-cp313-abi3-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Using cached brotli-1.2.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Using cached fastapi-0.125.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-1.0.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==2.0.1 (from gradio)\n",
      "  Using cached gradio_client-2.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub<2.0,>=0.33.5 (from gradio)\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (2.3.5)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (2.12.4)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Using cached python_multipart-0.0.21-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (6.0.3)\n",
      "Collecting safehttpx<0.2.0,>=0.1.7 (from gradio)\n",
      "  Using cached safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Using cached starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (0.17.4)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio) (4.15.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from gradio-client==2.0.1->gradio) (2025.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Using cached annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: shellingham in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Using cached gradio-6.1.0-py3-none-any.whl (23.0 MB)\n",
      "Using cached gradio_client-2.0.1-py3-none-any.whl (55 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached audioop_lts-0.2.2-cp313-abi3-win_amd64.whl (30 kB)\n",
      "Using cached fastapi-0.125.0-py3-none-any.whl (112 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Using cached safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Using cached annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached brotli-1.2.0-cp313-cp313-win_amd64.whl (369 kB)\n",
      "Using cached python_multipart-0.0.21-py3-none-any.whl (24 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: pydub, brotli, semantic-version, python-multipart, orjson, hf-xet, groovy, ffmpy, audioop-lts, annotated-doc, aiofiles, uvicorn, typer-slim, starlette, safehttpx, huggingface-hub, fastapi, gradio-client, gradio\n",
      "\n",
      "   -- -------------------------------------  1/19 [brotli]\n",
      "   ------ ---------------------------------  3/19 [python-multipart]\n",
      "   ------------ ---------------------------  6/19 [groovy]\n",
      "   --------------------- ------------------ 10/19 [aiofiles]\n",
      "   ----------------------- ---------------- 11/19 [uvicorn]\n",
      "   ----------------------- ---------------- 11/19 [uvicorn]\n",
      "   ------------------------- -------------- 12/19 [typer-slim]\n",
      "   --------------------------- ------------ 13/19 [starlette]\n",
      "   --------------------------- ------------ 13/19 [starlette]\n",
      "   --------------------------- ------------ 13/19 [starlette]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   ------------------------------- -------- 15/19 [huggingface-hub]\n",
      "   --------------------------------- ------ 16/19 [fastapi]\n",
      "   --------------------------------- ------ 16/19 [fastapi]\n",
      "   --------------------------------- ------ 16/19 [fastapi]\n",
      "   ----------------------------------- ---- 17/19 [gradio-client]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ------------------------------------- -- 18/19 [gradio]\n",
      "   ---------------------------------------- 19/19 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 annotated-doc-0.0.4 audioop-lts-0.2.2 brotli-1.2.0 fastapi-0.125.0 ffmpy-1.0.0 gradio-6.1.0 gradio-client-2.0.1 groovy-0.1.2 hf-xet-1.2.0 huggingface-hub-1.2.3 orjson-3.11.5 pydub-0.25.1 python-multipart-0.0.21 safehttpx-0.1.7 semantic-version-2.10.0 starlette-0.50.0 typer-slim-0.20.0 uvicorn-0.38.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899163ae-b087-470f-bf8f-38659675578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Using cached ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from ollama) (2.12.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mayur\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Using cached ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1256b97-3687-4024-a1cc-f97c74f72d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2125, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1607, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 1066, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mayur\\AppData\\Local\\Temp\\ipykernel_16452\\2712465043.py\", line 31, in eda_analysis\n",
      "    insights = generate_ai_insights(summary)\n",
      "  File \"C:\\Users\\mayur\\AppData\\Local\\Temp\\ipykernel_16452\\2712465043.py\", line 63, in generate_ai_insights\n",
      "    response = ollama.chat(\n",
      "        model=\"mistral\",\n",
      "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
      "    )\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\ollama\\_client.py\", line 365, in chat\n",
      "    return self._request(\n",
      "           ~~~~~~~~~~~~~^\n",
      "      ChatResponse,\n",
      "      ^^^^^^^^^^^^^\n",
      "    ...<14 lines>...\n",
      "      stream=stream,\n",
      "      ^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\ollama\\_client.py\", line 189, in _request\n",
      "    return cls(**self._request_raw(*args, **kwargs).json())\n",
      "                 ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mayur\\anaconda3\\Lib\\site-packages\\ollama\\_client.py\", line 133, in _request_raw\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model 'mistral' not found (status code: 404)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # Visualizations\n",
    "    plot_paths = generate_visualizations(df)\n",
    "\n",
    "    report = f\"\"\"\n",
    "DATASET LOADED SUCCESSFULLY ‚úÖ\n",
    "\n",
    "üìä SUMMARY:\n",
    "{summary}\n",
    "\n",
    "‚ùå MISSING VALUES:\n",
    "{missing_values}\n",
    "\n",
    "ü§ñ AI INSIGHTS:\n",
    "{insights}\n",
    "\"\"\"\n",
    "\n",
    "    return report, plot_paths\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# AI INSIGHTS (OLLAMA + MISTRAL)\n",
    "# ===============================\n",
    "def generate_ai_insights(df_summary):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional data scientist.\n",
    "Analyze the dataset summary and provide insights,\n",
    "patterns, correlations, and conclusions.\n",
    "\n",
    "{df_summary}\n",
    "\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DATA VISUALIZATION\n",
    "# ===============================\n",
    "def generate_visualizations(df):\n",
    "    plot_paths = []\n",
    "\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "    # Histograms\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(df[col], bins=30, kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        path = f\"plots/{col}_distribution.png\"\n",
    "        plt.savefig(path)\n",
    "        plot_paths.append(path)\n",
    "        plt.close()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    if not numeric_df.empty:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            numeric_df.corr(),\n",
    "            annot=True,\n",
    "            cmap=\"coolwarm\",\n",
    "            fmt=\".2f\"\n",
    "        )\n",
    "        plt.title(\"Correlation Heatmap\")\n",
    "        path = \"plots/correlation_heatmap.png\"\n",
    "        plt.savefig(path)\n",
    "        plot_paths.append(path)\n",
    "        plt.close()\n",
    "\n",
    "    return plot_paths\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# GRADIO APP (JUPYTER SAFE)\n",
    "# ===============================\n",
    "demo = gr.Interface(\n",
    "    fn=eda_analysis,\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üìë EDA Report\", lines=25),\n",
    "        gr.Gallery(label=\"üìà Visualizations\")\n",
    "    ],\n",
    "    title=\"üö¢ Titanic Dataset - LLM Powered EDA\",\n",
    "    description=\"Run inside Jupyter Notebook using Gradio + Ollama\"\n",
    ")\n",
    "\n",
    "demo.launch(inline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdc0f6-79f5-4298-ab90-78fed61d841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_analysis():\n",
    "    file_path = r\"C:\\Users\\mayur\\Downloads\\titanic_ dataset_final.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c57ea4-7234-43b4-86a2-5f84f70af830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d684bc-4274-4b7f-9fd6-b7cfcd699be5",
   "metadata": {},
   "outputs": [],
   "source": [
    " for col in df.select_dtypes(include=['number']).columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b1eae-aec9-4467-beb7-06d33e92e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.describe(include='all').to_string()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b897e3a-0f57-4594-ace0-3cf9190648eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "  missing_values = df.isnull().sum().to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031269d-0449-4727-b6a3-68dbd96567dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = generate_ai_insights(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0e671-48cf-4457-8091-9315c6df18bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
